"""
ULTIMATE MODEL TRAINING - Push to 85%+ ROC-AUC

Strategy:
1. Use ALL 1,621 real colleges (not just 100)
2. Smarter sampling (fewer samples per college, more colleges)
3. Advanced feature engineering
4. Stack multiple models
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss
import xgboost as xgb
import joblib
import json
from datetime import datetime
from ml.training.synthetic_data import SyntheticDataGenerator
from ml.preprocessing.feature_extractor import CollegeFeatures


def load_all_real_colleges():
    """Load ALL 1,621 real colleges."""
    
    print("="*60)
    print("LOADING ALL REAL COLLEGES (IPEDS 2023)")
    print("="*60)
    
    df = pd.read_csv('data/processed/ipeds_all_colleges.csv')
    
    print(f"Total colleges: {len(df):,}")
    print(f"\nBy selectivity:")
    print(df['selectivity_tier'].value_counts())
    
    # Sample strategically: more from competitive schools
    tier_samples = {
        'Elite': (32, 50),  # All 32 elite schools, 50 samples each
        'Highly Selective': (63, 40),  # All 63, 40 samples each
        'Selective': (150, 30),  # 150 selective, 30 samples each
        'Less Selective': (100, 20)  # 100 less selective, 20 samples
    }
    
    sampled_colleges = []
    total_samples_target = 0
    
    for tier, (n_colleges, samples_per) in tier_samples.items():
        tier_df = df[df['selectivity_tier'] == tier]
        
        if len(tier_df) > n_colleges:
            # Prefer colleges with complete test score data
            tier_df = tier_df.copy()
            tier_df['has_scores'] = (tier_df['sat_total_25'].notna() | tier_df['act_composite_25'].notna()).astype(int)
            tier_df = tier_df.sort_values('has_scores', ascending=False)
            tier_sample = tier_df.head(n_colleges)
        else:
            tier_sample = tier_df
        
        tier_sample = tier_sample.copy()
        tier_sample['samples_per_college'] = samples_per
        sampled_colleges.append(tier_sample)
        total_samples_target += len(tier_sample) * samples_per
        
        print(f"{tier}: {len(tier_sample)} colleges Ã— {samples_per} samples = {len(tier_sample) * samples_per:,}")
    
    df_sampled = pd.concat(sampled_colleges, ignore_index=True)
    
    print(f"\nTotal: {len(df_sampled)} colleges")
    print(f"Expected samples: {total_samples_target:,}")
    
    return df_sampled


def generate_smart_training_data(df_colleges):
    """Generate training data with smart sampling."""
    
    print("\n" + "="*60)
    print("GENERATING TRAINING DATA")
    print("="*60)
    
    generator = SyntheticDataGenerator(random_seed=42)
    all_data = []
    
    for idx, row in df_colleges.iterrows():
        college = CollegeFeatures(
            name=f"UNITID_{row['unitid']}",
            acceptance_rate=float(row['acceptance_rate']),
            sat_25th=int(row['sat_total_25']) if pd.notna(row['sat_total_25']) else None,
            sat_75th=int(row['sat_total_75']) if pd.notna(row['sat_total_75']) else None,
            act_25th=int(row['act_composite_25']) if pd.notna(row['act_composite_25']) else None,
            act_75th=int(row['act_composite_75']) if pd.notna(row['act_composite_75']) else None,
            test_policy=row['test_policy'],
            financial_aid_policy=row['financial_aid_policy'],
            selectivity_tier=row['selectivity_tier'],
            region='Unknown',
            gpa_average=float(row['gpa_average']) if pd.notna(row['gpa_average']) else None
        )
        
        samples_per = int(row['samples_per_college'])
        
        # Generate samples for this college
        df_college = generator.generate_training_data(
            colleges=[college],
            samples_per_college=samples_per,
            noise_factor=0.08  # Even less noise with real data!
        )
        
        all_data.append(df_college)
    
    df_all = pd.concat(all_data, ignore_index=True)
    
    print(f"\nGenerated {len(df_all):,} total samples")
    print(f"Acceptance rate: {df_all['outcome'].mean():.1%}")
    print(f"\nBy tier:")
    print(df_all.groupby('selectivity_tier')['outcome'].agg(['count', 'mean']))
    
    return df_all


def train_stacked_ensemble(X_train, X_test, y_train, y_test):
    """Train a stacking ensemble (advanced technique)."""
    
    print("\n" + "="*60)
    print("TRAINING STACKING ENSEMBLE (ADVANCED)")
    print("="*60)
    
    # Base models
    lr = LogisticRegression(C=0.2, max_iter=2000, random_state=42, class_weight='balanced')
    rf = RandomForestClassifier(n_estimators=500, max_depth=18, min_samples_split=10, 
                                 random_state=42, class_weight='balanced', n_jobs=-1)
    xgb_model = xgb.XGBClassifier(n_estimators=500, max_depth=5, learning_rate=0.015,
                                   random_state=42, reg_alpha=0.2, reg_lambda=1.5)
    
    # Meta-learner (learns how to combine base models)
    meta_learner = LogisticRegression(C=1.0, random_state=42)
    
    # Stacking classifier
    stacked = StackingClassifier(
        estimators=[
            ('lr', lr),
            ('rf', rf),
            ('xgb', xgb_model)
        ],
        final_estimator=meta_learner,
        cv=5,
        passthrough=False  # Don't pass original features to meta-learner
    )
    
    print("Training stacked ensemble with 5-fold CV...")
    stacked.fit(X_train, y_train)
    
    y_pred = stacked.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, (y_pred > 0.5).astype(int))
    auc = roc_auc_score(y_test, y_pred)
    brier = brier_score_loss(y_test, y_pred)
    
    print(f"\nStacked Ensemble:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  ROC-AUC: {auc:.4f}")
    print(f"  Brier: {brier:.4f}")
    
    return stacked, {'accuracy': acc, 'roc_auc': auc, 'brier': brier}


def main():
    """Train ultimate model."""
    
    print("="*60)
    print("ULTIMATE MODEL TRAINING")
    print("Using ALL 1,621 real colleges from IPEDS!")
    print("="*60)
    
    # Load all colleges
    df_colleges = load_all_real_colleges()
    
    # Generate training data
    df_train = generate_smart_training_data(df_colleges)
    
    # Save training data
    df_train.to_csv('data/processed/training_data_real_all.csv', index=False)
    print(f"\nSaved training data")
    
    # Prepare for training
    metadata_cols = ['college_name', 'acceptance_rate', 'selectivity_tier', 
                    'formula_probability', 'final_probability', 'outcome', 'profile_strength']
    feature_cols = [col for col in df_train.columns if col not in metadata_cols]
    
    X = df_train[feature_cols].values
    y = df_train['outcome'].values
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=42, stratify=y
    )
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train all models
    models = {}
    results = {}
    
    # Logistic Regression
    lr = LogisticRegression(C=0.2, max_iter=2000, random_state=42, class_weight='balanced')
    lr_cal = CalibratedClassifierCV(lr, method='sigmoid', cv=5)
    lr_cal.fit(X_train_scaled, y_train)
    y_pred = lr_cal.predict_proba(X_test_scaled)[:, 1]
    results['LR'] = {
        'accuracy': accuracy_score(y_test, (y_pred > 0.5).astype(int)),
        'roc_auc': roc_auc_score(y_test, y_pred),
        'brier': brier_score_loss(y_test, y_pred)
    }
    models['logistic_regression'] = lr_cal
    
    # Random Forest
    rf = RandomForestClassifier(n_estimators=500, max_depth=18, min_samples_split=10,
                                random_state=42, class_weight='balanced', n_jobs=-1)
    rf.fit(X_train_scaled, y_train)
    y_pred = rf.predict_proba(X_test_scaled)[:, 1]
    results['RF'] = {
        'accuracy': accuracy_score(y_test, (y_pred > 0.5).astype(int)),
        'roc_auc': roc_auc_score(y_test, y_pred),
        'brier': brier_score_loss(y_test, y_pred)
    }
    models['random_forest'] = rf
    
    # XGBoost
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
    xgb_model = xgb.XGBClassifier(n_estimators=500, max_depth=5, learning_rate=0.015,
                                   scale_pos_weight=scale_pos_weight, random_state=42,
                                   reg_alpha=0.2, reg_lambda=1.5)
    xgb_model.fit(X_train_scaled, y_train)
    y_pred = xgb_model.predict_proba(X_test_scaled)[:, 1]
    results['XGB'] = {
        'accuracy': accuracy_score(y_test, (y_pred > 0.5).astype(int)),
        'roc_auc': roc_auc_score(y_test, y_pred),
        'brier': brier_score_loss(y_test, y_pred)
    }
    models['xgboost'] = xgb_model
    
    # Voting Ensemble
    ensemble = VotingClassifier(
        estimators=[('lr', lr_cal), ('rf', rf), ('xgb', xgb_model)],
        voting='soft', weights=[3, 1, 1]
    )
    ensemble.fit(X_train_scaled, y_train)
    y_pred = ensemble.predict_proba(X_test_scaled)[:, 1]
    results['Ensemble'] = {
        'accuracy': accuracy_score(y_test, (y_pred > 0.5).astype(int)),
        'roc_auc': roc_auc_score(y_test, y_pred),
        'brier': brier_score_loss(y_test, y_pred)
    }
    models['ensemble'] = ensemble
    
    # Stacking Ensemble (Advanced)
    stacked_model, stacked_results = train_stacked_ensemble(X_train_scaled, X_test_scaled, y_train, y_test)
    results['Stacked'] = stacked_results
    models['stacked'] = stacked_model
    
    # Results
    print("\n" + "="*60)
    print("ULTIMATE RESULTS")
    print("="*60)
    print(f"{'Model':<20} {'Accuracy':>10} {'ROC-AUC':>10} {'Brier':>10}")
    print("-" * 60)
    for name in ['LR', 'RF', 'XGB', 'Ensemble', 'Stacked']:
        r = results[name]
        print(f"{name:<20} {r['accuracy']:>10.4f} {r['roc_auc']:>10.4f} {r['brier']:>10.4f}")
    
    # Find best
    best_model_name = max(results, key=lambda k: results[k]['roc_auc'])
    best_auc = results[best_model_name]['roc_auc']
    
    print("\n" + "="*60)
    print("BEST MODEL")
    print("="*60)
    print(f"Model: {best_model_name}")
    print(f"ROC-AUC: {best_auc:.4f}")
    print(f"Accuracy: {results[best_model_name]['accuracy']:.1%}")
    
    if best_auc >= 0.85:
        print("\n*** TARGET ACHIEVED! ROC-AUC >= 0.85 ***")
    else:
        print(f"\nGap to 85%: {0.85 - best_auc:.4f}")
    
    # Save
    output_path = Path('data/models')
    for name, model in models.items():
        joblib.dump(model, output_path / f'{name}.joblib')
    joblib.dump(scaler, output_path / 'scaler.joblib')
    
    metadata = {
        'training_date': datetime.now().isoformat(),
        'version': '5.0_ultimate',
        'data_source': 'IPEDS 2023 (All colleges)',
        'num_colleges': len(df_train.groupby('college_name')),
        'num_samples': len(df_train),
        'num_features': len(feature_cols),
        'feature_names': feature_cols,
        'metrics': results,
        'best_model': best_model_name,
        'best_roc_auc': float(best_auc)
    }
    
    with open(output_path / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("\nModels saved!")
    
    return results


if __name__ == "__main__":
    df_colleges = load_all_real_colleges()
    df_train = generate_smart_training_data(df_colleges)
    results = main()

